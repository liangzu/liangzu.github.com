<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-03-20 äºŒ 19:08 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>On Understanding NNPCA1988</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Liangzu Peng" />
<link rel="stylesheet" href="./css/my-css.css" type="text/css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">On Understanding NNPCA1988</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org967c3b1">1. Preliminaries</a></li>
</ul>
</div>
</div>
<p>
This post is my mathematical re-implementation of <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.1839&amp;rep=rep1&amp;type=pdf">Neural Networks and
PCA: Learning Without Local Minima</a> (1989), ignoring the problem
setting and any related background, which you can read from the paper, but
detailing the missing mathematics. Here is the reason: The
mathematical facts known by
experts (hence proofs are omitted) are
usually unknown to the fresh students.
</p>

<p>
It requires some linear algebra, calculus, and <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Optimization</a> (first 3
chapter) to begin with.
</p>
<div id="outline-container-org967c3b1" class="outline-2">
<h2 id="org967c3b1"><span class="section-number-2">1</span> Preliminaries</h2>
<div class="outline-text-2" id="text-1">
<ol class="org-ol">
<li>The quadratic function \[F(z)=||c-Mz||_2^2=c^Tc-2c^TMz+z^TM^TMz\,,\] where \(c,z\in
   \mathbb{R}^n,M\in\mathbb{R}^{n\times n}\) is convex (I assume \(M\) is
square matrix for the sake of simplicity). A short proof
is given here:\[F(\theta_1 z_1 + \theta_2 z_2)= ||c-M(\theta_1
   z_1 + \theta_2
   z_2)||_2^2=||\theta_1(c-Mz_1)+\theta_2(c-Mz_2)||_2^2\leq\theta_1F(z_1)+\theta_2F(z_2)\,.\]</li>

<li>Since \(F(z)\) is convex, a point \(z\) is a <b>global minimum</b> of
\(F\) if and only if \(\nabla F(z)=0\), i.e., \[M^TMz=M^Tc\,.\]
To compute \(\nabla F(z)\), you have to realize that (or prove it:)
<ul class="org-ul">
<li>the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a> of \(f(b)=a^Tb\) <code>w.r.t</code> \(b\) is \(a\), and</li>
<li>the gradient of \(g(a)=a^TXa\) <code>w.r.t.</code> \(a\) is \(Xa\).</li>
</ul></li>

<li>If in addition \(M^TM\) is positive definite, then \(F\) is strictly
convex and the <b>unique minimum</b> of \(F\) is attained for \(z=(M^TM)^{-1}Mc\).</li>
</ol>
</div>
</div>
</div>
</body>
</html>
