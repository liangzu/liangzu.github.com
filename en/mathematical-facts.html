<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-04-24 äºŒ 15:44 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Mathematical Facts</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Liangzu Peng" />
<link rel="stylesheet" href="./css/my-css.css" type="text/css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mathematical Facts</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org53bab72">1. Linear Algebra and Matrix</a>
<ul>
<li><a href="#org098d0fd">1.1. Linear Algebra</a></li>
<li><a href="#org0091c8e">1.2. Matrix Calculus</a></li>
<li><a href="#org175a66f">1.3. Kronecker Product</a></li>
</ul>
</li>
<li><a href="#org19c57af">2. Optimization</a>
<ul>
<li><a href="#org41514e8">2.1. Theory</a></li>
<li><a href="#org7e861bf">2.2. Gradient Descent</a></li>
</ul>
</li>
<li><a href="#org83911a0">3. Misc</a>
<ul>
<li><a href="#orgb382430">3.1. Convolution and Cross Correlation for Images</a>
<ul>
<li><a href="#orga10a583">3.1.1. Reference</a></li>
</ul>
</li>
<li><a href="#org79dd174">3.2. On Making Practical Assumptions</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
The math documented here, for
which though some simple proofs are given, is by no means rigirous. I
believe that for curious readers this will not be a problem.
</p>
<div id="outline-container-org53bab72" class="outline-2">
<h2 id="org53bab72"><span class="section-number-2">1</span> Linear Algebra and Matrix</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org098d0fd" class="outline-3">
<h3 id="org098d0fd"><span class="section-number-3">1.1</span> Linear Algebra</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>\(tr(ABCD)=tr(BCDA)=tr(CDAB)=tr(DABC)\).</li>
</ul>
</div>
</div>

<div id="outline-container-org0091c8e" class="outline-3">
<h3 id="org0091c8e"><span class="section-number-3">1.2</span> Matrix Calculus</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>The gradient of \(f(b)=a^Tb\) is \(a\).</li>
<li>The gradient of \(f(a)=a^TXa\) is \((X^T+X)a\). (<i>hint</i>: \(a^TXa=\Sigma_{i,j}x_{ij}a_ia_j\).)</li>
<li>The gradient of \(f(x)=Ax\) is \(A\).</li>
</ul>
</div>
</div>

<div id="outline-container-org175a66f" class="outline-3">
<h3 id="org175a66f"><span class="section-number-3">1.3</span> Kronecker Product</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Kronecker Product, denoted by \(\otimes\), is a generalization of the <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> from
vectors to matrices. Check the <a href="https://en.wikipedia.org/wiki/Kronecker_product">wikipedia</a> to understand its definition.
</p>

<p>
One of the most important property of Kronecker Product is:
\[vec(AXB)=(B^T\otimes A)vec(X)\,,\]
where \(A,B,X\) are matrices and <code>vec</code> operation is described <a href="https://en.wikipedia.org/wiki/Vectorization_(mathematics)">in this page</a>.
Here I give a simplified yet generalizable proof for this equation.
</p>

<p>
Let \(A=[a_1,\dots,a_n]\in\mathbb{R}^{n\times
 n},B=[b_1,\dots,b_n]\in\mathbb{R}^{n\times
 n},X=[x_1,\dots,x_n]\in\mathbb{R}^{n\times n}\), where
\(a_i=[a_{1i},a_{2i},\dots,a_{ni}]\in\mathbb{R}^n\) for each
\(i=1,\dots,n\) is <code>i</code>-th column of \(A\)
and the same is true for \(b_i\) and \(x_i\) too.  \(A_i\) is used to denote the
<code>i</code>-th row of \(A\).
</p>

<p>
We will prove that
\[vec(AXB^T)=(B\otimes A)vec(X)\,.\]
</p>

<ul class="org-ul">
<li>Notice that the first \(n\) row of \(vec(AXB^T)=vec(A\Sigma_i
   x_ib_i^T)=vec(\Sigma_i Ax_ib_i^T)\), is flatly the first column of \(\Sigma_i Ax_ib_i^T\).</li>

<li>Notice also that the first \(n\) row of \((B\otimes A)vec(X)\), which
is a column vector, is
\(b_{11}Ax_1+b_{12}Ax_2+\cdots+b_{1n}Ax_n=\Sigma_i b_{1i}Ax_i\).</li>

<li>It is easy to verify that the first column of \(\Sigma_i Ax_ib_i^T\)
is \(\Sigma_i b_{1i}Ax_i\), as desired.</li>

<li>We finished the proof.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org19c57af" class="outline-2">
<h2 id="org19c57af"><span class="section-number-2">2</span> Optimization</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org41514e8" class="outline-3">
<h3 id="org41514e8"><span class="section-number-3">2.1</span> Theory</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>A twice-differentiable function \(f\) is said to be \(\mu\text{-strongly}\)
convex and \(l\text{-smooth}\) if \[0\preceq \mu I\preceq f(x)\preceq lI,
  \forall x.\]</li>
</ul>
</div>
</div>

<div id="outline-container-org7e861bf" class="outline-3">
<h3 id="org7e861bf"><span class="section-number-3">2.2</span> Gradient Descent</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Given a differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\), we want to
algorithmically find its
(local or global) minimum if any. Let's start with an
arbitrary point \(x_0\in \mathbb{R}^d\), and construct a sequence
\(x_0,x_1,\dots, x_n,\dots\), such that \[f(x_{t+1})\lt f(x_t) \text{ for each
$t=0,1,2,\dots$}\,.\]
To this end, we just simply let \[x_{t+1}=x_{t}+\lambda_t v^t\,,\] where
\(\lambda_t\) is a pre-assigned positive number, called <i>learning rate</i>
or <i>step size</i>, and \(v^t\in \mathbb{R}^d\) is such that
\(\nabla f(x_t)^T v_t\lt 0\). \(\nabla f(x_t)^T v_t\) is the <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/directional-derivative">directional
derivative</a> at point \(x_t\) along direction \(v_t\), and hence, provided that
\(\lambda_t\gt 0\) and \(\lambda_t\) is small enough, \(\nabla f(x_t)^T
v_t\lt 0\) will ensure that \[f(x_{t+1})=f(x_{t}+\lambda_t v^t)\lt f(x_t)\,.\]
</p>

<p>
The next to do is find \(\lambda_t\) and the direction \(v_t\) for each
point \(x_t\). \(\lambda_t\). \(v_t=-\frac{\nabla
f(x_t)}{||f(x_t)||_2}\) will do it.
</p>

<p>
To summarize, we can construct the
desired sequence by the following algorithm, known as <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient
descent</a>, \[x_{t+1}=x_{t}-\lambda_t \nabla f(x_t)\,.\]
</p>


<p>
Finally it remains to be asked that can this algorithmically
constructed sequence, or say, the gradient descent algorithm, can
actually lead us to the (local or global) minimum? The answer is no if
we know beforehand that the given function
\(f:\mathbb{R}^{d}\to\mathbb{R}\) has no minimum at all. But how do we
know whether a function has or doesn't have optimal points?
</p>
</div>
</div>
</div>

<div id="outline-container-org83911a0" class="outline-2">
<h2 id="org83911a0"><span class="section-number-2">3</span> Misc</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgb382430" class="outline-3">
<h3 id="orgb382430"><span class="section-number-3">3.1</span> Convolution and Cross Correlation for Images</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The fact here applies only to images. Convolution said here means "2D
discrete convolution".
</p>

<p>
Let \(h\in\mathbb{R}^{(2k+1)\times(2k+1)}\) be a filter and \(I\) an
image with its size larger than \(h\)'s. They will be stored in your
program as multi-dimensional arrays, and you may write code like
\(a=h[0,0]\) (access the first element in the memory for \(h\)) to access
them. For mathematical convinience, we assume that the first element
of \(h\) in the memory of your comtuper is \(h[-k, -k]\), or that \((0,0)\)
is the "center" of the filter \(h\). We also assume, for again
mathematical convinience, that the value of \(h[i,j]\) or \(I[i,j]\) is \(0\) if
the index is out of boundary (zero padding). Not everyone entering
into the realm of computer vision has a background in signal
processing, and explicitly stating these
two assumptions, or conventions, therefore makes formulas more
accessable and more understandable.
</p>


<p>
The cross correlation
\(G\) between \(h\) and \(I\), written as \(G=h\otimes I\), is defined by the
following formula: \[G[i,j]=\Sigma_{u=-k}^{k}\Sigma_{v=-k}^{k} h[u,v]
I[i+u,j+v]\,.\]
People who try to be clever may write it as
\[G[i,j]=\Sigma_{u=i-k}^{i+k}\Sigma_{v=i-k}^{i+k}h[u-i,v-j]I[u,v]\,,\]
as \[G[i,j]=\Sigma_{u,v}h[u-i,v-j]I[u,v]\,,\]
or even as \[G_{i,j}=\Sigma_{u,v}h_{u-i,v-j}I_{u,v}\,.\]
</p>

<p>
A crucial observation is that these four formulas are equivalent.
</p>

<p>
The convolution \(G'\) between \(h\) and \(I\), written as \(G'=h*I\), is
defined by the following formulas:
\[G'[i,j]=\Sigma_{u=-k}^{k}\Sigma_{v=-k}^{k} h[u,v]I_[i-u,j-v]\,,\]
\[G'[i,j]=\Sigma_{u=i-k}^{i+k}\Sigma_{v=i-k}^{i+k}h[i-u,j-v]I[u,v]\,,\]
\[G'[i,j]=\Sigma_{u,v}h[i-u,j-v]I[u,v]\,,\]
or \[G'_{i,j}=\Sigma_{u,v}h_{i-u,j-v}I_{u,v}\,.\]
</p>
</div>


<div id="outline-container-orga10a583" class="outline-4">
<h4 id="orga10a583"><span class="section-number-4">3.1.1</span> Reference</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=C3EEy8adxvc">Convolution and Cross Correlation</a> on Youtube</li>
<li><a href="https://www.amazon.com/Computer-Vision-Modern-Approach-2nd/dp/013608592X">Computer Vision: A Modern Approach</a>, Forsyth and Ponce</li>
<li><a href="http://szeliski.org/Book/">Computer Vision: Algorithms and Applications</a>, Szeliski</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org79dd174" class="outline-3">
<h3 id="org79dd174"><span class="section-number-3">3.2</span> On Making Practical Assumptions</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>(Suppose that \(\Sigma_{XX}\) and \(\Sigma_{YX}\) are of full rank&#x2026;)
Full rank matrices are dense and in a realistic environment with
noise and finite precision, we can always slightly perturb the
conditions so as to make \(\Sigma\) invertible and with distinct
eigenvalues (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.1839&amp;rep=rep1&amp;type=pdf">NNPCA1988</a>).</li>
</ul>
</div>
</div>
</div>
</div>
</body>
</html>
